---
title: "Classification Non Supevisée , introduction"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

## Qu’est-ce que la classification non supervisée ?

La classification non supervisée est une méthode d’analyse qui vise à
regrouper des objets (individus, produits, observations, etc.) en
classes homogènes, sans connaître à l’avance ces classes. Contrairement
à la classification supervisée, il n’y a pas de variable à prédire : on
cherche simplement à découvrir des structures cachées dans les données.

## Pourquoi faire de la classification ?

L’objectif est de réduire la complexité d’un jeu de données en mettant
en évidence des structures internes qui permettent de comprendre les
données : On simplifie la lecture et l’interprétation. On identifie des
groupes d’objets similaires.

On retrouve une logique proche de l’analyse factorielle, qui réduit la
dimension, mais ici on réduit la diversité des profils Pour classer, il
faut pouvoir mesurer la proximité entre objets : c’est le rôle de la
distance métrique (euclidienne, Manhattan, etc.).

« Qui dit distance métrique dit contrainte spatiale » Autrement dit,
chaque individu est un point dans un espace à plusieurs dimensions et la
distance traduit sa ressemblance avec un autre point.

## Matrice des distances / dissimilarités

La « structure » des données est une vue bien particulière sur les
objets. Elle est définie par une notion de distances entre les objets,
le principe de la classification non supervisée étant de regrouper des
objets similaires (donc peu distants, ou « proches ») et de distinguer
les objets différents ( « éloignés » les uns des autres). La
classification non supervisée fait donc appel à une notion de distance
(dont le choix est primordial) qui est la base de la classification
d'objets.

Les données d’entrée d’un algorithme de classification peuvent être :

\- un **tableau individus × variables** (n × p),

\- une **matrice de distances** entre individus.

La matrice des distances est donc un point de départ possible de tout
algorithme de classification. C’est une matrice carrée de taille n × n
qui contient toutes les distances entre paires d’objets.

Cette matrice peut être une matrice de :

\- distance , qui mesure l'écart entre deux objets (dans un espace
donné)

\- dissimilarité, qui mesure le degré de différence entre 2 objets
donnés

## Critères d’agrégation

En classification hiérarchique, on utilise un critère d’agrégation pour
décider comment fusionner les groupes. On ne se place plus au niveau des
individus, mais au niveau des aggrégats d'individus.

## Critère de classification

La classification d'objet peut être déterministe ou "dure", c'est à dire
qu'un objet ou individu va être assigné à l'appartenance à une classe
uniquement, ou elle peut être probabiliste ou "floue", c'est à dire que
l'appartenance de l'individu ou de l'objet à une classe sera associée à
une probabilité d'appartenance à cette classe, la somme de toutes les
probabilités d'appartenance d'un individu à l'ensemble des classes étant
de 1.

## Stratégie de classification

Il existe différents types de stratégie de classification : le
partitionnement direct et la classification ascendante hierarchique (et
les stratégies mixtes qui les combinent). En partitionnement direct, le
nombre de clusters K en lequel les objets seront divisés est déterminé à
l'avance.

La classification hiérarchique ascendante (CAH) est une méthode
agrégative, où on part de n classes (chaque individu seul), on fusionne
à chaque étape les classes les plus proches et on obtient un
dendrogramme (ou arbre hiérarchique) qui montre les niveaux de
regroupement, que l'on peut ensuite couper à différents niveaux pour
obtenir la finesse de la partition souhaitée.

Cette méthode est adaptée aux variables quantitatives (une variable
correspond à une dimension) et aux variables qualitatives (une modalité
correspond à une dimension).

## De la classification aux lois de mélanges

On s'intéresse au processus génératif de données, dans le cadre de lois
de mélanges de gaussiennes multidimensionnelles. On se demande si on
peut retrouver le processus qui a permis d'aboutir à la génération de
données dans un cas simple (mélange de gaussiennes
multidimensionnelles). L'objectif est d'appliquer des algorithmes
d'optimisation afin d'obtenir les paramètres les plus vraisemblables,
ayant pu permettre de générer ces données. On estime ces paramètres
grâce aux données elles-mêmes. Ces algorithmes d'optimisation sont basés
sur le théorème de Bayes. La probabilité d'appartenance à une classe
peut être modélisée comme une variable latente.

## Des lois de mélange aux topic modelling

Le *topic modelling* est une méthode de classification non supervisée
appliquée aux textes.

On cherche à comprendre le processus génératif d'un document, en
applicant les méthodes évoquées précédemment. L'objectif est de savoir
comment un document à été généré. Or, on peut définir un document comme
une distribution de topics (les sujets dont il est composé), associés à
des proportions spécifiques. De plus, un topic est défini comme une
distribution aléatoire de mots. On cherche a faire du reverse
engineering et retrouver, à partir de documents, les processus qui ont
permis leur génération.

Un **topic** (ou « sujet ») correspond à une **distribution de mots
récurrents** dans un corpus.

Les objectifs sont :

\- Estimer la **proportion de chaque topic dans chaque document**.\
- Identifier les **mots caractéristiques** associés à chaque topic.

La méthode générale : 1. Identifier les topics dans chaque texte.\
2. Nommer les variables latentes (topics) selon les mots les plus
fréquents.\
3. Fusionner les tableaux représentant les topics.\
4. Calculer les distances entre topics.\
5. Appliquer une analyse multidimensionnelle pour visualiser les
proximités.
