---
title: "seance 4"
author: "Anna Mathieu"
date: "`r Sys.Date()`"
output: html_document
---

## Génération de données

On s'intéresse aux processus génératif de données et aux lois de mélange de gaussiennes bidimensionnelles. Connaissant l'allure des données (histogramme des deux gaussiennes mélangées), on cherche à définir une solution d'initialisation de l'algorithme et d'alterner entre étapes d'expectation (calcul des probabilités d'appartenance de chaque point donné à l'une et l'autre gaussienne), et de maximisation (calcul à partir des probabilités d'appartenance à chaque gaussienne, des paramètres mis-à-jour des gaussiennes : µ et sigma), puis calcul des probabilités d'appartenance à chaque gaussienne avec paramètres mis à jour, et ré-estimation des paramètres des gaussiennes par pondération des probabilités d'appartenance de chaque point, jusqu'à convergence de l'algorithme (par rapport à un critère d'arrêt défini).

On commence par générer des un vecteur de données de taille 1 x 500 pour un autre groupe. Ce vecteur est généré à partir d'un mélange en proportions que l'on choisit de valeurs issues de deux lois normales de paramètres (mu, sigma) que l'on choisit.

On va créer une fonction pour générer automatiquement ce vecteur de taille 1x500 à partir des paramètres mu1, mu2 (moyennes des gaussiennes 1 et 2), sigma1, sigma2 (écarts types des gaussiennes 1 et 2), n1 et n2 (nombre de valeurs de la gaussienne 1 générées, resp. 2)

```{r}
# Fonction permettant de générer des données 
f <- function(mu1, mu2, s1, s2, n1, n2) {
  vec1<- rnorm(n1,mu1, s1)
  vec2<- rnorm(n2,mu2, s2)
  vec <- sample(c(vec1, vec2))
  
  hist(vec, freq = F)
  
  return(vec)
}

res <- f(0, 10, 2, 3, 300, 200)
res

# envoi du vecteur 
write.csv(res, 'res.csv')
```

## Chargement des données reçues du groupe 4

On reçoit d'un autre groupe de la classe un vecteur de taille 1x500, similairement généré par mélange de deux gaussiennes.

```{r}
# Visualisation des données du vecteur 
vec <- read.table("vecteur_4.csv", sep = ",")
vec <- vec$V2
vec <- vec[-1]
vec <- sapply(vec, as.numeric)

hist(vec, nclass = 50, freq = F)
```

# Initialisation de l'algorithme

On choisit des vecteurs de paramètres permettant d'initialiser l'algorithme. On choisit $N(14,5)$ et $N(26,10)$.

On va calculer en 3 points $x_1 = 12$ , $x_2 = 20$ , $x_3 = 26$ , la valeur de la fonction de densité des lois gaussiennes, d'une part avec les paramètres 1, d'autre part avec les paramètres 2.

La formule de la densité d'une gaussienne est :

$f(x) = \frac{1}{\sigma \ * \  \sqrt(2 \ * \pi)} * \exp (- \frac{1}{2}(\frac{x - \mu}{\sigma}) )\ $

Cette fonction appelée fonction de répartition de la loi gaussienne modélise l'histogramme cumulatif d'une distribution en cloche.

Elle donne en chaque point donné x ; $x \in (-\infty \  ; \ +\infty)$) la valeur de f(x) correspondante. On sait que par définition la probabilité qu'un x soit compris entre a \<= x \<= b correspond à la dérivée de f(x) (f'(x)) de a à b, c'est-à-dire l'aire sous la courbe de f de a à b.

On commence par calculer en trois points, définis plus hauts, la densité de la fonction en chacun de ces points, s'ils appartenaient à la gaussienne 1 et s'ils appartenaient à la gaussienne 2. On fixe en effet la classe comme s'il s'agissait de la classe réelle. Plus la densité est élevée, plus le point sera proche du 'pic' de la gaussienne (donc de sa moyenne), donc plus la probabilité que ce point appartienne à cette gaussienne sera élévée, car observée cette valeur de x sous l'hypothèse nulle sera très probable. Au contraire, si la densité est très faible, alors la probabilité qu'un tel x appartenant à cette distribution ait été observé sous l'hypothèse nul est très faible.

Si le rapport des densités d1/d2 pour un x donné est supérieur à 1, alors il est plus probable que le point x appartienne à la gaussienne 1 qu'à la gaussienne 2 (et respectivement à la gaussienne 2 si d1/d2 est inféieur à 1).

On définit une fonction qui calcule la densité de x selon les paramètres $\mu \ ; \ \sigma$ de la gaussienne.

```{r}
densite <- function(sigma, mu, x) {
  d <- (1/(sigma*sqrt(2*pi)))  * exp((-1/2)*  ((x-mu)/sigma)^2)
  return(d)
}

```

On définit les vecteurs de paramètres d'initialisation

```{r}
mu1.1 <- 14
mu2.1 <- 26
s1.1 <- 5
s2.1 <- 10
```

On calcule pour chacun des trois points choisis la densité de f(x) selon les vecteurs de paramètres d'initialisation choisis.

```{r}
# Pour x = 12
d.1.12 <- densite(sigma = s1.1, mu = mu1.1, x = 12)
d.2.12 <- densite(sigma = s2.1, mu = mu2.1, x = 12)
d.1.12/d.2.12  # > 1

# pour x = 20
d.1.20 <- densite(sigma = s1.1, mu = mu1.1, x = 20)
d.2.20 <- densite(sigma = s2.1, mu = mu2.1, x = 20)
d.1.20/d.2.20 # > 1

# pour x = 25
d.1.25 <- densite(sigma = s1.1, mu = mu1.1, x = 25)
d.2.25 <- densite(sigma = s2.1, mu = mu2.1, x = 25)
d.1.25/d.2.25 # < 1
```

Le point proche du pic de la gaussienne 1 ainsi que le point au milieu entre les deux pics des deux gaussiennes sont plus probables d'appartenir à la distribution de la gaussienne 1. Le point proche du pic de la gaussienne 2 appartient plus probablement à la gaussienne 2.

Or, on ne connait pas les paramètres réels des deux gaussiennes, on veut les estimer.

On va donc étendre à tout le vecteur de valeur l'étape E, permettant d'associer chaque valeur du vecteur avec une probabilité d'appartenir à la distribution 1 et respectivement à la distribution 2. On va ensuite appliquer une étape de l'algorithme M, permettant à partir des moyennes pondérées de recalculer une meilleure estimation des vecteurs de paramètres des gaussiennes 1 et 2, puis répéter ces deux opérations jusqu'à convergence de l'algorithme.

## Etape E

L'objectif de l'étape E est d'étiquetter chacune des valeurs du vecteur *res* avec une probabilité d'appartenance à chacune des gaussiennes associé à son vecteur de paramètre initialisé.

On va calculer en chaque point du vecteur, et donc en chaque x, la densité de la fonction si le point x appartient à la gaussienne 1, et idem si le point x appartient à la gaussienne 2. De cette façon, on fixe l'appartenance de x à chacune des deux gaussiennes, donc calculer cette densité revient à calculer : 

- P(A|C1) : la probabilité pour que un individu qui appartient à la classe 1 soit classé comme appartenant à la classe 1 
- et resp. P(A|C2) : la probabilité qu'un individu de la classe 2 soit classé comme appartenant à la classe 2 (bien classé)

A partir de tous les calculs de densité d1 et d2, en chaque point du vecteur, on peut calculer pour chaque point s'il est plus probable qu'il appartienne à la gaussienne 1 ou à la gausienne 2. Ainsi, sur l'ensemble des 500 valeurs du vecteur *res* , on peut calculer la probabilité à posteriori d'appartenir à la classe 1 (gaussienne 1) ou à la classe 2 (gausseinne 2). 

P(C1) : probabilité d'appartenir à la classe 1
P(C2) : probabilité d'appartenir à la classe 2 

En réalité, ces probabilités sont connues par la loi de génération des données : 
$P(C1) = \frac {n1}{n1 \ + \ n2} $ 

Or nous ne connaissons pas n1 et n2. Ainsi, nous estimons P(C1) et P(C2) à partir du nombre de valeurs du vecteur de données pour lequel $\frac{d1}{d2} > 1$ . La somme de ces valeurs correspond à P(C1).

Par la formule de bayes, on sait que : 

$P(C_1|A) = \frac{P(A|C_1) \ * \ P(C_1)}{\sum_K \  P(A|C_K) \ * \ P(C_K)} $

soit : 

$P(C_1|A) = \frac{P(A|C_1) \ * \ P(C_1)} {P(A|C_1) \ * \ P(C_1) \ + \ P(A|C_2) \ * \ P(C_2) }   $
 
 
avec : 

- $P(C_1|A)$ : proba qu'un individu classé comme appartenant à la gaussienne 1 appartienne réellement à la gaussienne 1
- $P(A|C_1)$ : proba qu'un individu appartenant réellement à C1 soit classé comme appartenant à C1 (resp. C2)
- $P(C_1)$ : probabilité d'appartenir à C1 (resp. C2)


```{r}
etapeE <- function (vec, mu1, mu2, s1, s2) {
  # la fonction etapeE prend en entrée : 
  #   - un vecteur de valeurs
  #   - les paramètres initialisés (ou réestimés) des deux gaussiennes 
  
  # initialisation de la matrice 
  mat <- matrix(0, nrow = length(vec), ncol=2)
  # cette matrice de longueur 500 (taille du vecteur vec) accueillera dans ses colonnes 1 et 2 les valeurs des probabilités que la valeur i du vecteur vec appartienne à la gaussienne 1 et à la gaussienne 2. Chaque ligne i de la matrice somme donc à 1 avec chaque élément des colonnes 1 et 2 compris entre 0 et 1.
  
  # On initialise deux vecteurs d1 et d2
  # d1 (resp. d2) accueillera les valeurs des densités de chacun des x du vecteur *vec* s'il appartient à la gausienne 1 (de paramètres mu1 et s1) (et resp. 2, de paramètres mu2 et s2)
  d1 <- NULL 
  d2 <- NULL
  
  # calcul des densités du vecteur vec pour g1 et g2
  for (i in 1:length(vec)){
    d1[i] <- densite(sigma = s1, mu = mu1, x = vec[i] )
    d2[i] <- densite(sigma = s2, mu = mu2, x = vec[i] )
  }
  
  # print(d1)  # test d1
  # print(d2)  # test d2
  
  # on va compter les d1/d2
  compteur1 <- NULL  
  for (i in 1:length(d1)){
    if (d1[i]>d2[i]) {
      compteur1[i] <- TRUE
    }
  }

  # print(compteur1) 
  # soit TRUE soit 'na' 
  
  C1 <- sum(!is.na(compteur1)) ; # print(C1) 
  C2 <- length(vec)-C1
  
  p.C1 <- C1 / (C1+C2)
  p.C2 <- C2 / (C1+C2)
  
  # print(p.C1)
  # print(p.C2)
  # P(A| C1)  = d1
  
  p.A.C1 <- d1
  p.A.C2 <- d2
  
  # Remplissage de la matrice *mat* avec les proba P(C1|A) et P(C2|A)
  for (i in 1:nrow(mat)) {
  mat[i,1] <- (p.A.C1[i] * p.C1) / (  p.A.C1[i]*p.C1 +    p.A.C2[i]*p.C2 )
    
  mat[i,2] <- ( p.A.C2[i] * p.C2) / ( p.A.C1[i]*p.C1 +    p.A.C2[i]*p.C2 )
    
  }
  
  return(list(mat, p.C1, p.C2))
  # on retourne la matrice contenant les proba d'appartenance de chaque individu à l'une et l'autre gaussiennes
}
```

Etape E - initialisation de l'algorithme 
```{r}
res.E.1 <- etapeE(vec, mu1.1, mu2.1, s1 = s1.1, s2 = s2.1)
# res.E.1[[1]]
```



## Etape M

Pour l'étape M, on part du vecteur de données et de la matrice obtenue à l'étape E précédente.

```{r}
etapeM <- function(vecteur,matrice){
  
  moyenne <- numeric(ncol(matrice))
  ecarts_type <- numeric(ncol(matrice))
  
  for (j in 1:ncol(matrice)){
    haut <- 0
    bas <- 0
    somme <- 0
    
    for (i in 1:nrow(matrice)){
      haut <-haut + vecteur[i]*matrice[i,j]
      bas <- bas + matrice[i,j]
    }
    
    moyenne[j] <- haut/bas
    
    for (i in 1:nrow(matrice)) {
      somme <- somme + matrice[i,j] * (vecteur[i] - moyenne[j])^2
    }
    ecarts_type[j] <- sqrt(somme / bas)
  }
  return(list(vec = vecteur,mu1 = moyenne[1], mu2 = moyenne[2], s1= ecarts_type[1], s2 = ecarts_type[2]))
}
```


Etape M - initialisation 
```{r}
res.M.1 <- etapeM(vec, res.E.1[[1]])
```


## Alternance des étapes E et M et convergence de l'algorithme

On créé une fonction qui, à partir des éléments suivants en entrée : 
- vec : vecteur de valeurs 
- nb_iter : nombre d'itérations de l'algorithme à réaliser
- mu1.init, mu2.init, s1.init, s2.init les valeurs initiales des gaussiennes démélangées

Cette fonction réalise l'étape E et calcule pour chaque valeur du vecteur *vec* les probabilités d'appartenance aux gaussiennes 1 et 2, comme vu précédemment. 

La fonction etapeM est ensuite utilisée pour calculer à partir de ce vecteur et de la matrice obtenue grâce à l'étape E; les moyennes et écarts types pondérés mis à jour, donc réestimés à cette étape.
Cette fonction sort une liste qui contient le vecteur ainsi que les couples de paramètres $(\mu_1 \ , \ \sigma_1)$ et $(\mu_2 \ , \ \sigma_2)$. 

A l'étape suivante, l'étape E est à nouveau réalisée à partir des valeurs mises à jour des paramètres des gaussiennes démélangées. Et les valeurs de ces paramètres sont réestimés à l'étape M avec la matrice des probabilités d'appartenance de chaque individu à chacune des gaussiennes dont les paramètres ont été estimés à l'étape M précédente. On réalise ces étapes jusqu'à atteindre le nombre d'itérations donné en entrée.

```{r}
etapesEM <- function (vec, nb_iter, mu1.init = mu1.1, mu2.init = mu2.1, s1.init = s1.1, s2.init = s2.1 ) {
  
  num_iter <- 0
  
  # initialisation
  cat("réalisation de l'étape :", num_iter)
  resE <- etapeE(vec, mu1.1, mu2.1, s1.1, s2.1)
  resM <- etapeM(vec, matrice = resE[[1]])
  cat("\tNouvelles valeurs : mu1 =",round(resM[[2]]),"|", "mu2 =",round(resM[[3]]),"|", "s1 = ",round(resM[[4]]),"|", "s2 =", round(resM[[5]]))
  num_iter= num_iter + 1

  # itérations
  while(num_iter!=nb_iter) {
  cat("\nréalisation de l'étape :", num_iter)
  resE <- etapeE(vec = resM[[1]], mu1 = resM[[2]], mu2 = resM[[3]], s1 = resM[[4]], s2 = resM[[5]])
  resM <- etapeM(vecteur = resM[[1]], matrice = resE[[1]])
  cat("\tNouvelles valeurs : mu1 =",round(resM[[2]]),"|", "mu2 =",round(resM[[3]]),"|", "s1 = ",round(resM[[4]]),"|", "s2 =", round(resM[[5]]))
  num_iter= num_iter + 1
  }
  
}
```


Test de la fonction d'itération 
```{r}
etapesEM(vec = vec, nb_iter = 100)
```


## Critère d'arrêt
On va établit un critère d'arrêt pour que l'algorithme soit stoppé quand ce critère d'arrêt est atteint, au lieu de devoir définir un nombre préalable d'itérations, pouvant être soit : 
- trop grand (dans ce cas l'algorithme continue à calculer les estimations des paramètres à chaque itérations alors que la convergence aurait été atteinte), ce qui perd du temps et utilise inutilement de la puissance computationnelle 
- ou trop petit (dans ce cas, l'algorithme s'arrête avant la convergence qui aurait eu lieu avec un nombre plus grand d'itérations), donc les paramètres les plus vraimsemblables des deux gaussiennes ne sont pas estimées.

On va définir comme critère d'arrêt que le log de la vraisemblance soit maximisé.

```{r}
etapesEM_autostopped <- function (vec, mu1.init = mu1.1, mu2.init = mu2.1, s1.init = s1.1, s2.init = s2.1 ) {
  
  num_iter <- 1
  loglik <- NULL
  
  # initialisation
  resE <- etapeE(vec, mu1.1, mu2.1, s1.1, s2.1)
  resM <- etapeM(vec, matrice = resE[[1]])

  loglik[num_iter] <- sum(log(resE[[2]] * dnorm(vec, resM[[2]], resM[[4]]) + 
                        resE[[3]] * dnorm(vec, resM[[3]], resM[[5]])))
  
  cat("Étape", num_iter, ": mu1 =", round(resM[[2]]), "| mu2 =", round(resM[[3]]),
      "| s1 =", round(resM[[4]]), "| s2 =", round(resM[[5]]),
      "| Log-Vraisemblance =", round(loglik[num_iter], 4), "\n")
  
    # Boucle d'itération EM
  repeat {
    num_iter <- num_iter + 1
    
    resE <- etapeE(vec, resM[[2]], resM[[3]], resM[[4]], resM[[5]])
    resM <- etapeM(vec, matrice = resE[[1]])
    
    loglik[num_iter] <- sum(log(resE[[2]] * dnorm(vec, resM[[2]], resM[[4]]) +
                                resE[[3]] * dnorm(vec, resM[[3]], resM[[5]])))
    
    cat("Étape", num_iter, ": mu1 =", round(resM[[2]]), "| mu2 =", round(resM[[3]]),
        "| s1 =", round(resM[[4]]), "| s2 =", round(resM[[5]]),
        "| Log-Vraisemblance =", round(loglik[num_iter], 4), "\n")
    
    # condition d'arrêt
    if (loglik[num_iter] <= loglik[num_iter - 1]) {
      cat("\nArrêt automatique (plus d'amélioration de la log-vraisemblance)\n")
      break
    }
  }
  
}
```

On test la variabilité du résultat avec des valeurs d'initialisation différentes.
```{r}
etapesEM_autostopped(vec)
```
```{r}
etapesEM_autostopped(vec, mu1.init = 17 , mu2.init = 25,s1.init = 0 , s2.init = 5)
etapesEM_autostopped(vec, mu1.init = 13 , mu2.init = 17,s1.init = 0.5 , s2.init = 0)
# même avec d'autres valeurs d'initialisation 

```


Vérification 
```{r}
x <- f(13,25,4,6, 400,100)
hist(x, freq = F, nclass = 50)
```


Test avec notre vecteur généré pour un autre groupe (res)
```{r}
etapesEM_autostopped(vec = res, mu1.init = 0 , mu2.init = 12, s1.init =1 , s2.init = 2)
```



## Conclusion

A partir de la programmation manuelle d'un algorithme itératif d'expectation-maximisation, il est possible de retrouver la loi de mélange de gaussiennes bidimensionnelles dont on peut estimer les paramètres comme l'a prouvé cet exemple. On peut donc s'intéresser à la génération de données. 




